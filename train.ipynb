{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c1f4106-3781-4b22-8878-19cef68294b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import torch\n",
    "\n",
    "from model import AttentionModel, init_weights\n",
    "from utils import get_all_files, get_lr, save_checkpoint, tokenize_files, get_random_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "314dc22c-319d-4cc4-b7a3-aa0a9b160eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE=device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "## CONSTANTS\n",
    "BATCH_SIZE = 16\n",
    "CONTEXT_SIZE = 1792\n",
    "VOCAB_SIZE = 1024\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"{DEVICE=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2e23b51-ca9d-4ba3-bb21-52e0204cbc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 216788\n",
      "Validation files: 4425\n"
     ]
    }
   ],
   "source": [
    "train_files = get_all_files(\"./data/train\")\n",
    "val_files = get_all_files(\"./data/val\")\n",
    "\n",
    "print(f\"Train files: {len(train_files)}\")\n",
    "print(f\"Validation files: {len(val_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e42923-1560-451d-9f25-b20c9f3b1dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "START_TOKEN = \"<|startoftext|>\"\n",
    "EOD_TOKEN = \"<|endoftext|>\"\n",
    "PAD_TOKEN = \"<|pad|>\"\n",
    "\n",
    "tokenizer.train(\n",
    "    files=train_files + val_files,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[EOD_TOKEN,PAD_TOKEN]\n",
    ")\n",
    "\n",
    "def encode(text):\n",
    "    return tokenizer.encode(text).ids\n",
    "\n",
    "def decode(digits):\n",
    "    return tokenizer.decode(digits)\n",
    "\n",
    "print(decode(encode(\"Hello, world!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b58f818f-c7d2-41dd-833e-380c599c7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary as a dictionary: {token: index}\n",
    "vocab_dict = tokenizer.get_vocab()\n",
    "\n",
    "# # Print the first 20 tokens (sorted by index)\n",
    "# sorted_vocab = sorted(vocab_dict.items(), key=lambda item: item[1])\n",
    "# for token, idx in sorted_vocab[:1000]:\n",
    "#     print(f\"{idx}: {repr(token)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35358f35-8ec4-476f-bd27-e568b338347f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|█████████████████████████████████████████████████████████████▏                      | 157781/216788 [02:20<00:56, 1046.50it/s]"
     ]
    }
   ],
   "source": [
    "lens = []\n",
    "train_files_clean = []\n",
    "for file in tqdm(train_files):\n",
    "    with open(file, \"r\", encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    text_encoded = encode(text)\n",
    "    lens.append(len(text_encoded))\n",
    "    if len(text_encoded)<CONTEXT_SIZE:\n",
    "        train_files_clean.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb3b7e8a-48cf-47c0-b389-4fc634d6b324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 4425/4425 [00:04<00:00, 1099.85it/s]\n"
     ]
    }
   ],
   "source": [
    "val_files_clean = []\n",
    "for file in tqdm(val_files):\n",
    "    with open(file, \"r\", encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    text_encoded = encode(text)\n",
    "    lens.append(len(text_encoded))\n",
    "    if len(text_encoded)<CONTEXT_SIZE:\n",
    "        val_files_clean.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26ce8ef4-88da-4cfc-a985-37bc4c5dc2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 4424/4424 [00:03<00:00, 1125.24it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "pad_id = encode(PAD_TOKEN)\n",
    "print(pad_id)\n",
    "\n",
    "for file in tqdm(train_files_clean):\n",
    "    with open(file, \"r\", encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    text_encoded = encode(text)\n",
    "    text_encoded += pad_id * (CONTEXT_SIZE - len(text_encoded))\n",
    "    train_data.append(text_encoded)\n",
    "\n",
    "for file in tqdm(val_files_clean):\n",
    "    with open(file, \"r\", encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    text_encoded = encode(text)\n",
    "    text_encoded += pad_id * (CONTEXT_SIZE - len(text_encoded))\n",
    "    val_data.append(text_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bab38d0f-5b5b-45df-b56e-c0f1d6c96b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.tensor(train_data).to(DEVICE)\n",
    "val_data = torch.tensor(val_data).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49276f99-7699-4a49-8abf-27d34e848168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(data, batch_size):\n",
    "    ix = torch.randint(len(data) , (batch_size,))\n",
    "    x = torch.stack([data[i][:-1] for i in ix])\n",
    "    y = torch.stack([data[i][1:] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dd89c2e-41f1-43c7-a7de-3ae9fac60c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(;GM[1]FF[4]SZ[19]PB[Black]PW[White]KM[0]RE[B+8.5]TM[60]TT;B[br];W[pp];B[dd];W[pd];B[nq];W[pn];B[jp];W[cq];B[eq];W[dq];B[ep];W[cn];B[nc];W[qf];B[pb];W\n",
      "**************************************************\n",
      "GM[1]FF[4]SZ[19]PB[Black]PW[White]KM[0]RE[B+8.5]TM[60]TT;B[br];W[pp];B[dd];W[pd];B[nq];W[pn];B[jp];W[cq];B[eq];W[dq];B[ep];W[cn];B[nc];W[qf];B[pb];W[\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_random_batch(train_data, BATCH_SIZE)\n",
    "\n",
    "print(decode(xb[0].tolist()[:100]))\n",
    "print(\"*\" * 50)\n",
    "print(decode(yb[0].tolist()[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3e4fa07-ec57-4b38-a0d3-ed09abe0656f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13535.0625"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ecd6438-e93f-42b2-a619-a5be85739de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_count=4\n",
      "4376*4=17504\n",
      "num_of_steps=61264\n",
      "warmup_steps=2000\n"
     ]
    }
   ],
   "source": [
    "epoch_count = 4\n",
    "num_of_steps = int(epoch_count * 4376 * 3.5)\n",
    "warmup_steps = 2000  # Warm up for first 2000 steps\n",
    "check_val_every = 500\n",
    "eval_count = 250\n",
    "\n",
    "lr_max = 3e-4  # Maximum learning rate\n",
    "lr_min = 5e-6  # Minimum learning rate\n",
    "total_steps = num_of_steps\n",
    "\n",
    "print(f\"{epoch_count=}\")\n",
    "print(f\"{4376*4=}\")\n",
    "print(f\"{num_of_steps=}\")\n",
    "print(f\"{warmup_steps=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15a79b65-d321-48bd-b31d-b6186bf63a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "hparam_search = [\n",
    "    # {\"att_size\": 512, \"head_count\": 8, \"dropout\": 0.1, \"layer_count\":8, \"gpt_init\":True},\n",
    "    # {\"att_size\": 1024, \"head_count\": 16, \"dropout\": 0, \"layer_count\":16, \"gpt_init\":False},\n",
    "    # {\"att_size\": 1024, \"head_count\": 16, \"dropout\": 0.1, \"layer_count\":16, \"gpt_init\":False},\n",
    "    # {\"att_size\": 1024, \"head_count\": 16, \"dropout\": 0.1, \"layer_count\":16, \"gpt_init\":True},\n",
    "    {\"att_size\": 768, \"head_count\": 12, \"dropout\": 0.1, \"layer_count\":12, \"gpt_init\":True},\n",
    "]\n",
    "hparams = hparam_search[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "525f5b0e-6f2c-48da-8a5e-b9b9e7900efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grad_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    return total_norm\n",
    "\n",
    "def compute_param_norm(model):\n",
    "    total_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            total_norm += p.data.norm(2).item() ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "# def compute_weight_update_norm(model, prev_params):\n",
    "#     total_update_norm = 0.0\n",
    "#     for p, prev_p in zip(model.parameters(), prev_params):\n",
    "#         if p.requires_grad:\n",
    "#             delta = (p.data - prev_p).norm(2).item()\n",
    "#             total_update_norm += delta ** 2\n",
    "#     return total_update_norm ** 0.5\n",
    "\n",
    "# compute_entropy_and_perplexity\n",
    "def training_step(model, train_tokens, context_size, batch_size):\n",
    "    # Time the step\n",
    "    step_start = time.time()\n",
    "\n",
    "    # Training Step\n",
    "    xb, yb = get_random_batch(train_tokens, batch_size)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        logits, loss = model(xb, yb)\n",
    "\n",
    "    entropy, perplexity = compute_entropy_and_perplexity(logits.float(), yb)\n",
    "    \n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "    if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
    "        raise ValueError(f\"!!! Invalid loss at step {step}: {loss.item()}\")\n",
    "    \n",
    "    # Important: unscale before clipping!\n",
    "    scaler.unscale_(optimizer)\n",
    "\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(m.parameters(), 1.0)\n",
    "    \n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # tmp_losses.append(loss.item())\n",
    "\n",
    "    # Update LR\n",
    "    current_lr = get_lr(step, total_steps, lr_max, lr_min, warmup_steps, num_of_steps*10) * 3\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = current_lr\n",
    "\n",
    "    step_time = time.time() - step_start\n",
    "    return loss.item(), grad_norm.item(), current_lr, entropy, perplexity, step_time\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_tokens, context_size, batch_size, eval_count):\n",
    "    model.eval()\n",
    "    tmp_eval_perplexity, tmp_eval_entropy = [], []\n",
    "    with torch.no_grad():\n",
    "        with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            tmp_test_losses = []\n",
    "            for _ in range(eval_count):\n",
    "                xb, yb = get_random_batch(val_tokens, batch_size)\n",
    "                test_logits, test_loss = m(xb, yb)\n",
    "                entropy, perplexity = compute_entropy_and_perplexity(test_logits, yb)\n",
    "                tmp_test_losses.append(test_loss.item())\n",
    "                tmp_eval_perplexity.append(perplexity)\n",
    "                tmp_eval_entropy.append(entropy)\n",
    "    test_loss_avg = sum(tmp_test_losses) / len(tmp_test_losses)\n",
    "    test_perplexity_avg = sum(tmp_eval_perplexity) / len(tmp_eval_perplexity)\n",
    "    test_entropy_avg = sum(tmp_eval_entropy) / len(tmp_eval_entropy)\n",
    "    return test_loss_avg, test_perplexity_avg, test_entropy_avg\n",
    "\n",
    "\n",
    "def plot_metrics(steps, losses, test_losses, learning_rates, grad_norms):\n",
    "    # --- Save figure ---\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(10, 10), gridspec_kw={'height_ratios': [3, 1, 2, 3]})\n",
    "    \n",
    "    # Full loss curves\n",
    "    ax1.plot(steps, losses, 'b-', label='Train Loss')\n",
    "    ax1.plot(steps, test_losses, 'y-', label='Test Loss')\n",
    "    ax1.set_title(f'Step {step} | Train {losses[-1]:.4f} | Test {test_losses[-1]:.4f}')\n",
    "    ax1.grid()\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Learning rate\n",
    "    ax2.plot(steps, learning_rates, 'g-')\n",
    "    ax2.set_title('Learning Rate')\n",
    "    ax2.grid()\n",
    "    \n",
    "    # Zoomed-in loss view (last 10 steps)\n",
    "    last_n = 10\n",
    "    ax3.plot(steps[-last_n:], losses[-last_n:], 'b-', label='Train Loss (Last 10)')\n",
    "    ax3.plot(steps[-last_n:], test_losses[-last_n:], 'y-', label='Test Loss (Last 10)')\n",
    "    ax3.set_title('Zoomed-in Loss View (Last 10 Steps)')\n",
    "    ax3.grid()\n",
    "    ax3.legend()\n",
    "    \n",
    "    ax4.plot(steps, grad_norms, 'y-', label='Grad Norm')\n",
    "    ax4.set_title(f'Step {step} | Last Norm {grad_norm:.4f} ')\n",
    "    ax4.grid()\n",
    "    ax4.legend()\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/step_{step}.png\")\n",
    "    clear_output(wait=True)  # Clear the previous plot\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def sample_generations(model, test_prompts, encode, device, temperature, max_new_tokens):\n",
    "    generations = []\n",
    "    for prompt in test_prompts:\n",
    "        input_matrix = torch.concat(([torch.tensor([encode(prompt)])])).to(device)\n",
    "        generation = model.generate(input_matrix, max_new_tokens=max_new_tokens, temperature=temperature).tolist()\n",
    "        generations += generation\n",
    "    for generation in generations:\n",
    "        print(decode(generation))\n",
    "        print(\"\\n********\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "727409c3-9745-4582-b219-b85cd231ee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "num_of_steps=61264\n"
     ]
    }
   ],
   "source": [
    "# --- Model ---\n",
    "m = AttentionModel(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    att_size=hparams[\"att_size\"],\n",
    "    head_count=hparams[\"head_count\"],\n",
    "    layer_count= hparams[\"layer_count\"],\n",
    "    context_size=CONTEXT_SIZE,\n",
    "    drop_out=hparams[\"dropout\"]\n",
    ")\n",
    "\n",
    "if hparams[\"gpt_init\"]:\n",
    "    m.apply(init_weights)\n",
    "m = m.to(DEVICE)\n",
    "m = torch.compile(m)\n",
    "print(next(m.parameters()).device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(),  betas=(0.9, 0.95),  lr=1e-3, weight_decay=0.1)\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "steps = []\n",
    "losses = []\n",
    "test_losses = []\n",
    "learning_rates = []\n",
    "tmp_losses = []\n",
    "step_times = []\n",
    "grad_norms = []\n",
    "\n",
    "tmp_train_perplexity, tmp_train_entropy = [], []\n",
    "\n",
    "start = time.time()\n",
    "print(f\"{num_of_steps=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f492cfe8-acf5-4561-9f51-5762dbbcdc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_entropy_and_perplexity(logits, targets):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: (batch_size, seq_len, vocab_size) — raw model outputs\n",
    "        targets: (batch_size, seq_len) — ground-truth token ids\n",
    "        pad_token_id: Optional — to mask out padding tokens in loss\n",
    "\n",
    "    Returns:\n",
    "        avg_entropy: average token-level entropy\n",
    "        perplexity: exp(avg_cross_entropy_loss)\n",
    "    \"\"\"\n",
    "    # Flatten for easier computation\n",
    "    vocab_size = logits.size(-1)\n",
    "    logits_flat = logits.view(-1, vocab_size)\n",
    "    targets_flat = targets.view(-1)\n",
    "\n",
    "    # Compute log probs\n",
    "    log_probs = F.log_softmax(logits_flat, dim=-1)\n",
    "    probs = log_probs.exp()\n",
    "\n",
    "    # Cross-entropy loss (used for perplexity)\n",
    "    ce_loss = F.nll_loss(log_probs, targets_flat, reduction='none')  # (num_tokens,)\n",
    "\n",
    "    # Entropy of each prediction\n",
    "    entropy = -(probs * log_probs).sum(dim=-1)  # (num_tokens,)\n",
    "\n",
    "    avg_ce = ce_loss.mean().item()\n",
    "    avg_entropy = entropy.mean().item()\n",
    "    perplexity = torch.exp(torch.tensor(avg_ce)).item()\n",
    "\n",
    "    return avg_entropy, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c78ad384-892a-4d56-9d1c-75fdb0b9df0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Model: NVIDIA GeForce RTX 5090\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/baduk-gpt-pytorch/wandb/run-20250525_144231-hw9acbzs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/seigenai333-gena-co/baduk-llm-dev-2/runs/hw9acbzs' target=\"_blank\">stoic-frog-3</a></strong> to <a href='https://wandb.ai/seigenai333-gena-co/baduk-llm-dev-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/seigenai333-gena-co/baduk-llm-dev-2' target=\"_blank\">https://wandb.ai/seigenai333-gena-co/baduk-llm-dev-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/seigenai333-gena-co/baduk-llm-dev-2/runs/hw9acbzs' target=\"_blank\">https://wandb.ai/seigenai333-gena-co/baduk-llm-dev-2/runs/hw9acbzs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU Model: {device_name}\")\n",
    "else:\n",
    "    device_name = \"CPU\"\n",
    "    print(\"CUDA is not available.\")\n",
    "    \n",
    "import wandb\n",
    "run = wandb.init(\n",
    "    entity=\"entity_name\",\n",
    "    project=\"project_name\",\n",
    "    config={\n",
    "        \"architecture\": \"GPT-2-V1\",\n",
    "        \"dataset\": \"sgfs\",\n",
    "        \"device_name\": device_name,\n",
    "        **hparams\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ac508-573c-417e-8262-2658aed0ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = 1\n",
    "test_prompts = [\n",
    "    \"(\",\n",
    "    \"(\",\n",
    "    \"(\"\n",
    "]\n",
    "times = []\n",
    "for step in range(0, num_of_steps):\n",
    "    loss, grad_norm, current_lr, train_entropy, train_perplexity, step_time = training_step(m, train_data, CONTEXT_SIZE, BATCH_SIZE)\n",
    "\n",
    "    tmp_train_entropy.append(train_entropy)\n",
    "    tmp_train_perplexity.append(train_perplexity)\n",
    "\n",
    "    tmp_losses.append(loss)\n",
    "    step_times.append(step_time)\n",
    "\n",
    "    # Every 100 steps: Evaluate + Save\n",
    "    if step % check_val_every == 0:\n",
    "        grad_norms.append(grad_norm)\n",
    "        \n",
    "        current_loss = sum(tmp_losses) / len(tmp_losses)\n",
    "        avg_train_entropy = sum(tmp_train_entropy)/len(tmp_train_entropy)\n",
    "        avg_train_perplexiy = sum(tmp_train_perplexity)/len(tmp_train_perplexity)\n",
    "\n",
    "        tmp_losses, tmp_train_entropy, tmp_train_perplexity = [], [], []\n",
    "\n",
    "        test_loss_avg, test_perplexity_avg, test_entropy_avg = evaluate_model(m, val_data, CONTEXT_SIZE, BATCH_SIZE, eval_count)\n",
    "        \n",
    "        steps.append(step)\n",
    "        losses.append(current_loss)\n",
    "        test_losses.append(test_loss_avg)\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "        param_norm = compute_param_norm(m)\n",
    "\n",
    "        run.log({\n",
    "            \"train loss\": current_loss,\n",
    "            \"val loss\": test_loss_avg,\n",
    "            \"learning rate\":current_lr,\n",
    "            \"grad norm\": grad_norm,\n",
    "            \"param_norm\":param_norm,\n",
    "            \"train_entropy\":avg_train_entropy,\n",
    "            \"train_perplexity\":avg_train_perplexiy,\n",
    "            \"test_entropy\":test_entropy_avg,\n",
    "            \"test_perplexity\":test_perplexity_avg,\n",
    "        })\n",
    "\n",
    "        plot_metrics(steps, losses, test_losses, learning_rates, grad_norms)\n",
    "\n",
    "        # sample_generations(m, test_prompts, encode, DEVICE, temperature=0.2)\n",
    "\n",
    "        m.train()\n",
    "\n",
    "        # --- Save partial result ---\n",
    "        partial_result = {\n",
    "            \"run_id\": run_id,\n",
    "            \"step\": step,\n",
    "            \"hparams\": hparams,\n",
    "            \"train_loss\": current_loss,\n",
    "            \"test_loss\": test_loss_avg,\n",
    "            \"lr\": current_lr,\n",
    "            \"avg_step_time_sec_last_100\": sum(step_times[-check_val_every:]) / len(step_times[-check_val_every:]),\n",
    "        }\n",
    "        with open(f\"results/partial_run_{run_id}_step_{step}.json\", \"w\") as f:\n",
    "            json.dump(partial_result, f, indent=2)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# Final Save\n",
    "final_result = {\n",
    "    \"run_id\": run_id,\n",
    "    \"hparams\": hparams,\n",
    "    \"final_train_loss\": losses[-1],\n",
    "    \"final_val_loss\": test_losses[-1],\n",
    "    \"duration_sec\": total_time,\n",
    "    \"avg_step_time_sec\": sum(step_times) / len(step_times),\n",
    "}\n",
    "with open(f\"results/final_result_{run_id}.json\", \"w\") as f:\n",
    "    json.dump(final_result, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b1a531-5ff3-4bbe-9819-4cc38ba04fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a41c6-fd7e-49f6-ba3c-8bb452872d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_generations(m, test_prompts, encode, DEVICE,0.4, 1700)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
